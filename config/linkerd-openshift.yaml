################################################################################
# Linkerd Sevice Mesh
#
# This is a basic Kubernetes config file to deploy a service mesh of Linkerd
# instances onto your Kubernetes cluster that is capable of handling HTTP,
# HTTP/2 and gRPC calls with some reasonable defaults.
#
# To configure your applications to use Linkerd for HTTP traffic you can set the
# `http_proxy` envionment variable to `$(NODE_NAME):4140` where `NODE_NAME` is
# the name of node on which the application instance is running.  The
# `NODE_NAME` environment variable can be set with the downward API.
#
# If your application does not support the `http_proxy` environment variable or
# if you want to configure your application to use Linkerd for HTTP/2 or gRPC
# traffic, you must configure your application to send traffic directly to
# Linkerd:
#
# * $(NODE_NAME):4140 for HTTP
# * $(NODE_NAME):4240 for HTTP/2
# * $(NODE_NAME):4340 for gRPC
#
# If you are sending HTTP or HTTP/2 traffic directly to Linkerd, you must set
# the Host/Authority header to `<service>` or `<service>.<namespace>` where
# `<service>` and `<namespace>` are the names of the service and namespace
# that you want to proxy to.  If unspecified, `<namespace>` defaults to
# `default`.
#
# If your application receives HTTP, HTTP/2, and/or gRPC traffic it must have a
# Kubernetes Service object with ports named `http`, `h2`, and/or `grpc`
# respectively.
#
# You can deploy this to your Kubernetes cluster by running:
#   kubectl create ns linkerd
#   kubectl apply -n linkerd -f servicemesh.yml
#
# There are sections of this config that can be uncommented to enable:
# * CNI compatibility
# * Automatic retries
# * Zipkin tracing
################################################################################
#---
#apiVersion: v1
#kind: ConfigMap
#metadata:
#  name: l5d-config
#  namespace: linkerd
#data:
#  config.yaml: |-
#    admin:
#      port: 9990
#
#    # Namers provide Linkerd with service discovery information.  To use a
#    # namer, you reference it in the dtab by its prefix.  We define 4 namers:
#    # * /io.l5d.k8s gets the address of the target app
#    # * /io.l5d.k8s.http gets the address of the http-incoming Linkerd router on the target app's node
#    # * /io.l5d.k8s.h2 gets the address of the h2-incoming Linkerd router on the target app's node
#    # * /io.l5d.k8s.grpc gets the address of the grpc-incoming Linkerd router on the target app's node
#    namers:
#    - kind: io.l5d.k8s
#    - kind: io.l5d.k8s
#      prefix: /io.l5d.k8s.grpc
#      transformers:
#        # The daemonset transformer replaces the address of the target app with
#        # the address of the grpc-incoming router of the Linkerd daemonset pod
#        # on the target app's node.
#      - kind: io.l5d.k8s.daemonset
#        namespace: linkerd
#        port: grpc-incoming
#        service: l5d
#        # hostNetwork: true # Uncomment if using host networking (eg for CNI)
#    - kind: io.l5d.rewrite
#      prefix: /portNsSvcToK8s
#      pattern: "/{port}/{ns}/{svc}"
#      name: "/k8s/{ns}/{port}/{svc}"
#
#    # Routers define how Linkerd actually handles traffic.  Each router listens
#    # for requests, applies routing rules to those requests, and proxies them
#    # to the appropriate destinations.  Each router is protocol specific.
#    # For each protocol (HTTP, HTTP/2, gRPC) we define an outgoing router and
#    # an incoming router.  The application is expected to send traffic to the
#    # outgoing router which proxies it to the incoming router of the Linkerd
#    # running on the target service's node.  The incoming router then proxies
#    # the request to the target application itself.  We also define HTTP and
#    # HTTP/2 ingress routers which act as Ingress Controllers and route based
#    # on the Ingress resource.
#    routers:
#    - label: gprc-incoming
#      protocol: h2
#      experimental: true
#      servers:
#      - port: 4341
#        ip: 0.0.0.0
#      identifier:
#        kind: io.l5d.header.path
#        segments: 1
#      interpreter:
#        kind: default
#        transformers:
#        - kind: io.l5d.k8s.localnode
#          # hostNetwork: true # Uncomment if using host networking (eg for CNI)
#      dtab: |
#        /srv => /#/io.l5d.k8s/default/grpc ;             # /srv/service/package -> /#/io.l5d.k8s/default/grpc/service/package
#        /svc => /$/io.buoyant.http.domainToPathPfx/srv ; # /svc/package.service -> /srv/service/package
#
#    # HTTP/2 Ingress Controller listening on port 8080
#    - protocol: h2
#      experimental: true
#      label: h2-ingress
#      servers:
#        - port: 8080
#          ip: 0.0.0.0
#          clearContext: true
#      identifier:
#        kind: io.l5d.ingress
#      dtab: /svc => /#/io.l5d.k8s

---
apiVersion: v1
kind: DeploymentConfig
metadata:
  labels:
    app: l5d
  name: l5d
  namespace: myproject
spec:
  template:
    metadata:
      labels:
        app: l5d
    spec:
      # hostNetwork: true # Uncomment to use host networking (eg for CNI)
      replicas: 1
      selector:
        app: l5d
      strategy:
        activeDeadlineSeconds: 21600
        resources: {}
        rollingParams:
        intervalSeconds: 1
        maxSurge: 25%
        maxUnavailable: 25%
        timeoutSeconds: 600
        updatePeriodSeconds: 1
        type: Rolling
      volumes:
      - name: l5d-config
        configMap:
          name: "l5d-config"
      containers:
      - name: l5d
        image: norse:5000/linkerd:1.1.3
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        args:
        - /io.buoyant/linkerd/config/config.yaml
        ports:
        - name: http-outgoing
          containerPort: 4140
          hostPort: 4140
        - name: http-incoming
          containerPort: 4141
        - name: h2-outgoing
          containerPort: 4240
          hostPort: 4240
        - name: h2-incoming
          containerPort: 4241
        - name: grpc-outgoing
          containerPort: 4340
          hostPort: 4340
        - name: grpc-incoming
          containerPort: 4341
        - name: http-ingress
          containerPort: 80
        - name: h2-ingress
          containerPort: 8080
        volumeMounts:
        - name: "l5d-config"
          mountPath: "/io.buoyant/linkerd/config"
          readOnly: true

      # Run `kubectl proxy` as a sidecar to give us authenticated access to the
      # Kubernetes API.
      - name: kubectl
        image: buoyantio/kubectl:v1.4.0
        args:
        - "proxy"
        - "-p"
        - "8001"
---
apiVersion: v1
kind: Service
metadata:
  name: l5d
  namespace: myproject
spec:
  selector:
    app: l5d
  type: LoadBalancer
  ports:
  - name: http-outgoing
    port: 4140
  - name: http-incoming
    port: 4141
  - name: h2-outgoing
    port: 4240
  - name: h2-incoming
    port: 4241
  - name: grpc-outgoing
    port: 4340
  - name: grpc-incoming
    port: 4341
  - name: http-ingress
    port: 80
  - name: h2-ingress
    port: 8080